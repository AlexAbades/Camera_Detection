{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EAST Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imutils\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image and check it's correct loaded \n",
    "im1 = cv2.imread(\"data/spa/spa1_01MO.jpg\")\n",
    "cv2.imshow('Image 1', im1)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EAST detector only allows multiples of 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predefine width and height\n",
    "im_w = 320 \n",
    "im_h = 320 \n",
    "\n",
    "# Create a copy of the image \n",
    "im1_c = im1.copy()\n",
    "H, W, _ = im1_c.shape\n",
    "\n",
    "# Calculate the ratio in change for both directions\n",
    "r_W = H / im_w\n",
    "r_H = W / im_w\n",
    "\n",
    "# Resize the image to the prespecified dimensions \n",
    "im1_r = cv2.resize(im1_c, (im_w, im_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the new image \n",
    "cv2.imshow('Image 1', im1_r)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to decrease a lot the image and see if the detection speed improves. We have to iterate through all the columns and rows to detect changes in intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the EAST algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "layersNames = [\n",
    "    \"feature_fusion/Conv_7/Sigmoid\",  # Layer to out the probabilities (Confidence of the text box)\n",
    "    \"feature_fusion/concat_3\"         # Derive the bounding box coordinates (Geometry of the text box)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNet(\"frozen_east_text_detection.pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = cv2.dnn.blobFromImage(im1_r, 1.0, (im_w, im_h), (123.68, 116.78, 103.94), True, False)\n",
    "# The first argument is the image itself\n",
    "# The second argument specifies the scaling of each pixel value. Not required, keep it 1.\n",
    "# The default input to the network is 320Ã—320. So, we need to specify this while creating the blob.\n",
    "# We also specify the mean that should be subtracted from each image, this was used while training the model\n",
    "# The mean used is (123.68, 116.78, 103.94).\n",
    "# The next argument is whether we want to swap the R and B channels. This is required since OpenCV uses BGR \n",
    "# format and Tensorflow uses RGB format.\n",
    "# The last argument is whether we want to crop the image and take the center crop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track time \n",
    "start = time.time()\n",
    "# construct a blob from the image and then perform a forward pass of\n",
    "# the model to obtain the two output layer sets\n",
    "net.setInput(blob)\n",
    "scores, geometry = net.forward(layersNames)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time 0.2644991874694824\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average time {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 80, 80)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[boxes, confidences] = decode(scores, geometry, confThreshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e52dddd8ca2ba95afe67578a96296e9b17628fddb050e9ee950fdfaca96878c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
